{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e6b6a3",
   "metadata": {},
   "source": [
    "You can try your own prompt with differnet model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbfec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# HuggingFace model name (or local path)\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Hidden stats json path (must contain obj[\"summary\"])\n",
    "stats_json_path = Path(\"flores_hidden_stats_llama2_7b_50.json\")  # change if needed\n",
    "\n",
    "# Device / dtype\n",
    "device = \"cuda\"   # \"cuda\" or \"cpu\"\n",
    "dtype  = \"float16\"  # \"float16\" | \"bfloat16\" | \"float32\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(\"model_name:\", model_name)\n",
    "print(\"stats_json_path:\", stats_json_path.resolve())\n",
    "print(\"device:\", device, \"| dtype:\", dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fa74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model and wrapping\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from repeng import ControlVector, ControlModel\n",
    "from repeng.control import ControlModule\n",
    "\n",
    "def resolve_dtype(dtype: str) -> torch.dtype:\n",
    "    if dtype == \"float16\":\n",
    "        return torch.float16\n",
    "    if dtype == \"bfloat16\":\n",
    "        return torch.bfloat16\n",
    "    return torch.float32\n",
    "\n",
    "def load_summary(stats_json_path: Path):\n",
    "    with stats_json_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    return obj[\"summary\"]\n",
    "\n",
    "def patch_control_forward_overwrite_nonzero():\n",
    "    \"\"\"\n",
    "    Overwrite ControlModule.forward with overwrite-nonzero behavior.\n",
    "    Uses ControlModule.custom_coeff as a scalar multiplier.\n",
    "    \"\"\"\n",
    "    ControlModule.custom_coeff = 1.0\n",
    "\n",
    "    def normalized_forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        control = self.params.control\n",
    "\n",
    "        if control is None:\n",
    "            return output\n",
    "        if len(control.shape) == 1:\n",
    "            control = control.reshape(1, 1, -1)\n",
    "        if torch.all(control == 0):\n",
    "            return output\n",
    "\n",
    "        modified = output[0] if isinstance(output, tuple) else output\n",
    "        control = control.to(modified.device)\n",
    "\n",
    "        norm_pre = torch.norm(modified, dim=-1, keepdim=True)\n",
    "\n",
    "        # Padding mask handling (optional)\n",
    "        if \"position_ids\" in kwargs:\n",
    "            pos = kwargs[\"position_ids\"]\n",
    "            zero_idx = (pos == 0).cumsum(1).argmax(1, keepdim=True)\n",
    "            col = torch.arange(pos.size(1), device=pos.device).unsqueeze(0)\n",
    "            mask = (col >= zero_idx).float().reshape(modified.shape[0], modified.shape[1], 1)\n",
    "            mask = mask.to(modified.dtype).to(modified.device)\n",
    "        else:\n",
    "            mask = 1.0\n",
    "\n",
    "        control_applied = control * float(ControlModule.custom_coeff) * mask\n",
    "        modified = torch.where(control_applied != 0, control_applied, modified)\n",
    "\n",
    "        if self.params.normalize:\n",
    "            norm_post = torch.norm(modified, dim=-1, keepdim=True)\n",
    "            modified = modified / norm_post * norm_pre\n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            return (modified,) + output[1:]\n",
    "        return modified\n",
    "\n",
    "    ControlModule.forward = normalized_forward\n",
    "\n",
    "# ---- Load summary ----\n",
    "summary = load_summary(stats_json_path)\n",
    "\n",
    "# ---- Load tokenizer/model ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\" if device.startswith(\"cuda\") else None,\n",
    "    torch_dtype=resolve_dtype(dtype),\n",
    ")\n",
    "base_model.eval()\n",
    "\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ---- Patch repeng forward ----\n",
    "patch_control_forward_overwrite_nonzero()\n",
    "\n",
    "# ---- Wrap model with ControlModel ----\n",
    "num_layers = len(base_model.model.layers)\n",
    "last_layer_id = num_layers - 1\n",
    "\n",
    "wrap_start_layer = 6  # you can keep this fixed for playground\n",
    "wrapped_layers = list(range(wrap_start_layer, num_layers))\n",
    "model = ControlModel(base_model, wrapped_layers)\n",
    "\n",
    "print(\"Loaded model:\", model_name)\n",
    "print(\"num_layers:\", num_layers, \"| last_layer_id:\", last_layer_id)\n",
    "print(\"wrapped layers:\", wrapped_layers[0], \"...\", wrapped_layers[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aaac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your prompt and settings\n",
    "# Setting: \"para\" or \"mono\"\n",
    "setting = \"para\"\n",
    "anchor_layer = 19  # only used when setting == \"mono\" (1-based)\n",
    "\n",
    "\n",
    "target_lang = \"zh\"         # target language for EN->X\n",
    "k_dims = 400               # top-K dims\n",
    "\n",
    "# Intervention layer (1-based)\n",
    "intervention_layer_1based = 19\n",
    "# Strength list\n",
    "strength_list = [0.4, 0.8, 1.2]\n",
    "\n",
    "# Decoding params\n",
    "max_new_tokens = 64\n",
    "\n",
    "do_sample = False\n",
    "temperature = 0.0\n",
    "top_p = 0.9\n",
    "repetition_penalty = 1.1\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Translate an English sentence into target language.\\nEnglish: I love Kyoto in winter.\\nTarget language: \"\n",
    "\n",
    "print(\"setting:\", setting, \"| target_lang:\", target_lang, \"| k_dims:\", k_dims)\n",
    "print(\"intervention_layer_1based:\", intervention_layer_1based, \"| strengths:\", strength_list)\n",
    "print(\"do_sample:\", do_sample, \"| temperature:\", temperature, \"| top_p:\", top_p, \"| max_new_tokens:\", max_new_tokens)\n",
    "print(\"prompt:\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from repeng import ControlVector\n",
    "from repeng.control import ControlModule\n",
    "\n",
    "anchor_layer-=1\n",
    "def apply_model_specific_zeroing(diff: np.ndarray, model_name: str):\n",
    "    # keep your special handling if needed; here only llama2-7b / 13b examples\n",
    "    if \"Llama-2-7b\" in model_name or \"Llama-2-7b-hf\" in model_name:\n",
    "        if diff.shape[0] > 1415: diff[1415] = 0.0\n",
    "        if diff.shape[0] > 2533: diff[2533] = 0.0\n",
    "    if \"Llama-2-13b\" in model_name or \"Llama-2-13b-hf\" in model_name:\n",
    "        if diff.shape[0] > 2100: diff[2100] = 0.0\n",
    "\n",
    "def build_control_vec_en2x(summary, setting, lang, k_dims, last_layer_id, anchor_layer):\n",
    "    \"\"\"\n",
    "    EN->X:\n",
    "      para: source=en@last, target=lang@last\n",
    "      mono: source=lang@anchor, target=lang@last\n",
    "    Use top-K dims by |source-target|, and set vector to target_mean on those dims.\n",
    "    \"\"\"\n",
    "    tgt_key = f\"layer_{last_layer_id}\"\n",
    "    target_mean = np.asarray(summary[lang][tgt_key][\"mean\"], dtype=np.float32)\n",
    "\n",
    "    if setting == \"mono\":\n",
    "        src_key = f\"layer_{anchor_layer}\"\n",
    "        source_mean = np.asarray(summary[lang][src_key][\"mean\"], dtype=np.float32)\n",
    "    else:\n",
    "        source_mean = np.asarray(summary[\"en\"][tgt_key][\"mean\"], dtype=np.float32)\n",
    "\n",
    "    diff = source_mean - target_mean\n",
    "    apply_model_specific_zeroing(diff, model_name)\n",
    "\n",
    "    topk_idx = np.argsort(-np.abs(diff))[:k_dims]\n",
    "    control_vec = np.zeros_like(target_mean, dtype=np.float32)\n",
    "    control_vec[topk_idx] = target_mean[topk_idx]\n",
    "    return control_vec\n",
    "\n",
    "def truncate_at_stop(s: str) -> str:\n",
    "    idx = s.find(\"\\n\")\n",
    "    return s if idx == -1 else s[:idx]\n",
    "\n",
    "# ----- Validate layer (1-based -> 0-based) -----\n",
    "layer_idx = int(intervention_layer_1based) - 1\n",
    "if not (0 <= layer_idx < num_layers):\n",
    "    raise ValueError(f\"intervention_layer_1based must be in [1, {num_layers}], got {intervention_layer_1based}\")\n",
    "\n",
    "# ----- Build control vector for target_lang -----\n",
    "control_vec = build_control_vec_en2x(\n",
    "    summary=summary,\n",
    "    setting=setting,\n",
    "    lang=target_lang,\n",
    "    k_dims=k_dims,\n",
    "    last_layer_id=last_layer_id,\n",
    "    anchor_layer=anchor_layer,\n",
    ")\n",
    "\n",
    "# ----- Decode kwargs (avoid warnings if greedy) -----\n",
    "gen_kwargs = dict(\n",
    "    do_sample=do_sample,\n",
    "    repetition_penalty=float(repetition_penalty),\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=int(max_new_tokens),\n",
    ")\n",
    "if do_sample:\n",
    "    gen_kwargs[\"temperature\"] = float(temperature)\n",
    "    gen_kwargs[\"top_p\"] = float(top_p)\n",
    "\n",
    "# ----- Prepare prompt -----\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "\n",
    "# ----- Run for each strength -----\n",
    "for strength in strength_list:\n",
    "    # Apply control at exactly one layer\n",
    "    layer_directions = {\n",
    "        lid: (control_vec if lid == layer_idx else np.zeros_like(control_vec))\n",
    "        for lid in range(num_layers)\n",
    "    }\n",
    "    control_vector = ControlVector(model_type=\"llama\", directions=layer_directions)\n",
    "\n",
    "    model.reset()\n",
    "    model.set_control(control_vector, coeff=1.0)\n",
    "\n",
    "    ControlModule.custom_coeff = float(strength)\n",
    "\n",
    "    out = model.generate(**inputs, **gen_kwargs)\n",
    "    gen_tokens = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "    text = truncate_at_stop(text).strip()\n",
    "\n",
    "    print(f\"Intervention layer: {intervention_layer_1based} | Strength: {float(strength):.2f}\")\n",
    "    print(f\"Response: {text}\\n\")\n",
    "\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
